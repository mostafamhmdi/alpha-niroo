{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11158163,"sourceType":"datasetVersion","datasetId":6962154}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, MultiHeadAttention, LayerNormalization","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install mplfinance\n! pip install pandas_ta","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/eurusd/EURUSD-30-DS.csv', header=None, names=['Date','Open', 'High', 'Low', 'Close','Volume'])\ndata.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas_ta as ta\ndata['RSI_14'] = ta.rsi(data['Close'], length=14)\ndata['MA20'] = ta.ema(data['Close'], length=20)\ndata['MACD'] = ta.macd(data['Close'])['MACD_12_26_9']\ndata['ATR_4'] = ta.atr(data['High'], data['Low'], data['Close'], length=4)\ndata['ATR_14'] = ta.atr(data['High'], data['Low'], data['Close'], length=14)\ndata['ATR_40'] = ta.atr(data['High'], data['Low'], data['Close'], length=40)\nbb = ta.bbands(data['Close'], length=20)\ndata['BB_upper'] = bb['BBU_20_2.0']\ndata['BB_lower'] = bb['BBL_20_2.0']\ndata['Stoch_K'] = ta.stoch(data['High'], data['Low'], data['Close'])['STOCHk_14_3_3']\ndata['CCI_20'] = ta.cci(data['High'], data['Low'], data['Close'], length=20)\ndata['adx_14'] = data.ta.adx(length=14)['ADX_14']\ndata['Candle_Range'] = data['High'] - data['Low']\ndata['Candle_Body'] = data['Close'] - data['Open']\n\n\n# # اضافه کردن لگ‌های زمانی برای Average\n# for lag in range(1, 6):  # لگ‌های 1 تا 5\n#     data[f'Average_Lag_{lag}'] = data['Average'].shift(lag)\n\ndata['Rolling_Mean_10'] = data['Close'].rolling(window=10).mean()\ndata['Rolling_Std_20'] = data['Close'].rolling(window=20).std()\ndata['Rolling_Max_20'] = data['High'].rolling(window=20).max()\ndata['Rolling_Min_20'] = data['Low'].rolling(window=20).min()\ndata['Price_Change'] = (data['Close'] - data['Open']) / data['Open']\ndata['Stoch_D'] = ta.stoch(data['High'], data['Low'], data['Close'])['STOCHd_14_3_3']\ndata = data.dropna()\n\ndata['Date'] = pd.to_datetime(data['Date'], format='%d.%m.%Y %H:%M:%S.%f')\ndata['Hour_of_Day'] = data['Date'].dt.hour\ndata['Day_of_Week'] = data['Date'].dt.dayofweek\n\ndata['Average'] = ((data['Open'] + data['Close'] + data['High'] + data['Low']) / 4)\n\ndata = data.dropna()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\ncorrelation_matrix = data[['Open', 'High', 'Low', 'Close', 'Volume', 'RSI_14', 'MA20',\n       'MACD', 'ATR_4', 'ATR_14', 'ATR_40', 'BB_upper', 'BB_lower', 'Stoch_K',\n       'CCI_20', 'adx_14', 'Candle_Range', 'Candle_Body', 'Rolling_Mean_10',\n       'Rolling_Std_20', 'Rolling_Max_20', 'Rolling_Min_20', 'Price_Change',\n       'Stoch_D', 'Hour_of_Day', 'Day_of_Week', 'Average']].corr()\nplt.figure(figsize=(25, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix Heatmap')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_sequences(inputs, outputs, seq_length):\n    X, y = [], []\n    for i in range(len(inputs) - seq_length):\n        X.append(inputs[i:i + seq_length])\n        y.append(outputs[i + seq_length])  # 4 مقدار OHLC تایم‌فریم بعدی\n    return np.array(X), np.array(y)\n\nseq_length = 20\nX, y = create_sequences(scaled_inputs, scaled_outputs, seq_length)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_size = int(len(X) * 0.8)\nval_size = int(len(X) * 0.1)\nX_train, X_val, X_test = X[:train_size], X[train_size:train_size + val_size], X[train_size + val_size:]\ny_train, y_val, y_test = y[:train_size], y[train_size:train_size + val_size], y[train_size + val_size:]\n\nprint(\"تعداد نمونه‌های Train:\", len(X_train))\nprint(\"تعداد نمونه‌های Validation:\", len(X_val))\nprint(\"تعداد نمونه‌های Test:\", len(X_test))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv1D, LSTM, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nimport numpy as np\n\ninputs = Input(shape=(seq_length, len(num_features))\n\n# لایه‌های CNN\nx = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\nx = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\n\n# لایه‌های LSTM\nx = LSTM(64, return_sequences=True)(x)\nx = Dropout(0.4)(x)\nx = BatchNormalization()(x)\nx = LSTM(32, return_sequences=False)(x)\nx = Dropout(0.4)(x)\n\n# لایه‌های خروجی\nx = Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.05))(x)\nx = Dropout(0.3)(x)\noutputs = Dense(1)(x)\n\n# تعریف مدل\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# بهینه‌ساز و کامپایل\noptimizer = Adam(learning_rate=0.001)  # نرخ یادگیری اولیه بالاتر\nmodel.compile(optimizer=optimizer, loss='mae', metrics=['mae'])\n\n# زمان‌بندی نرخ یادگیری\ndef lr_scheduler(epoch, lr):\n    if epoch < 5:\n        return float(lr)\n    else:\n        return float(lr * np.exp(-0.05))\n\nlr_scheduler_callback = LearningRateScheduler(lr_scheduler)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\nearly_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# خلاصه مدل\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# آموزش مدل\nhistory = model.fit(\n    X_train, y_train,\n    epochs=70,\n    batch_size=32,\n    validation_data=(X_val, y_val),\n    callbacks=[early_stop, lr_scheduler_callback],\n    verbose=1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Loss in training')\nplt.xlabel('Epoch')\nplt.ylabel('Loss (MSE)')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['mae'], label='Train MAE')\nplt.plot(history.history['val_mae'], label='Validation MAE')\nplt.title('MAE in training')\nplt.xlabel('Epoch')\nplt.ylabel('MAE')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = model.predict(X_test)\n\n# 10. برگردوندن به مقیاس اصلی\npredictions_average = scalers_outputs['Average'].inverse_transform(predictions[:, 0].reshape(-1, 1))\n\n\ny_test_average = scalers_outputs['Average'].inverse_transform(y_test[:, 0].reshape(-1, 1))\n\n# 11. محاسبه معیارها برای هر مقدار\ndef calculate_metrics(y_true, y_pred, name):\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    print(f\"\\nمعیارهای ارزیابی برای {name}:\")\n    print(f\"MSE: {mse}\")\n    print(f\"RMSE: {rmse}\")\n    print(f\"MAE: {mae}\")\n    print(f\"R² Score: {r2}\")\n\ncalculate_metrics(y_test_average, predictions_average, \"Average\")\n\n\n\n# 13. رسم پیش‌بینی‌ها\nplt.figure(figsize=(20, 6))\nplt.subplot(1, 1, 1)\nplt.plot(y_test_average, label='real', color='blue')\nplt.plot(predictions_average, label='predicted', color='red',linestyle='--')\nplt.title('Average: actual vs predicted')\nplt.legend()\n\n\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_samples = 1000\nn_samples_per_plot = 100  # تعداد نمونه‌ها در هر نمودار\nn_plots = total_samples // n_samples_per_plot  # تعداد نمودارها (10)\n\n# تنظیمات شکل کلی\nplt.figure(figsize=(12, 8 * n_plots))  # ارتفاع شکل رو متناسب با تعداد نمودارها تنظیم می‌کنیم\n\n# حلقه برای رسم 10 نمودار\nfor i in range(n_plots):\n    # محاسبه بازه نمونه‌ها برای هر نمودار\n    start_idx = i * n_samples_per_plot\n    end_idx = (i + 1) * n_samples_per_plot\n    \n    # انتخاب داده‌ها برای این بازه\n    y_test_subset = y_test_average[start_idx:end_idx]\n    predictions_subset = predictions_average[start_idx:end_idx]\n    \n    # ایجاد زیرنمودار\n    plt.subplot(n_plots, 1, i + 1)  # n_plots ردیف، 1 ستون، اندیس i+1\n    \n    # رسم خطوط\n    plt.plot(y_test_subset, label='real', color='blue')\n    plt.plot(predictions_subset, label='predicted', color='red')\n    \n    # اضافه کردن عنوان و لیبل‌ها\n    plt.title(f'Average: actual vs predicted (Samples {start_idx} to {end_idx})')\n    plt.ylabel('Average')\n    plt.legend()\n    \n    # تنظیم محور x برای نمایش اندیس نمونه‌ها\n    plt.xticks(np.arange(0, n_samples_per_plot, 10))  # تیک‌ها هر 10 نمونه\n\n# تنظیمات نهایی و نمایش نمودار\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import mplfinance as mpf\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# فرض می‌کنیم data دیتافریم اولیه شماست که ستون‌های Date, Open, High, Low, Close, Average داره\n# تبدیل ستون Date به فرمت datetime\ndata['Date'] = pd.to_datetime(data['Date'], format='mixed', dayfirst=True)\n\n# تعداد نمونه‌های تست رو از X_test می‌گیریم\nn_test_samples = len(X_test)  # تعداد نمونه‌های تست: 21186\n\n# انتخاب داده‌های تست از دیتافریم data\ntest_data = data.iloc[-n_test_samples:].copy()\ntest_data.set_index('Date', inplace=True)\ntest_data.index = pd.DatetimeIndex(test_data.index)\n\n# محدود کردن تعداد نمونه‌ها برای رسم (100 نمونه اول)\nn_samples_to_plot = 100\ntest_data = test_data.iloc[:n_samples_to_plot]  # 100 نمونه اول\n\n# فرض می‌کنیم predictions خروجی مدل شماست و برای داده‌های تست (X_test) هست\n# predictions باید ابعاد (n_test_samples, 1) داشته باشه\npredicted_average = predictions.flatten()  # تبدیل به آرایه یک‌بعدی\npredicted_average = predicted_average[:n_samples_to_plot]  # محدود کردن به 100 نمونه اول\n\n# اگه predictions نرمال‌سازی‌شده هست، باید به مقیاس اصلی برگردونیم\n# فرض می‌کنیم scalers_outputs['Average'] همون اسکیلری هست که برای نرمال‌سازی Average استفاده کردی\npredicted_average = scalers_outputs['Average'].inverse_transform(predicted_average.reshape(-1, 1)).flatten()\n\n# ایجاد یک DataFrame برای Average پیش‌بینی‌شده با همان ایندکس\npredicted_df = pd.DataFrame({\n    'Predicted_Average': predicted_average\n}, index=test_data.index)\n\n# برای مقایسه، Average واقعی رو هم از test_data می‌گیریم\nactual_average = test_data['Average'].values\nactual_average_df = pd.DataFrame({\n    'Actual_Average': actual_average\n}, index=test_data.index)\n\n# محاسبه معیارهای ارزیابی\nmae = mean_absolute_error(actual_average, predicted_average)\nrmse = np.sqrt(mean_squared_error(actual_average, predicted_average))\nprint(f\"MAE on Test Data (First 100 Samples): {mae}\")\nprint(f\"RMSE on Test Data (First 100 Samples): {rmse}\")\n\n# تنظیمات برای رسم\nfig = mpf.figure(figsize=(15, 10))\nax = fig.add_subplot(1, 1, 1)\n\n# رسم کندل‌استیک‌ها\nmpf.plot(test_data, type='candle', style='charles', ax=ax, volume=False)\n\n# اضافه کردن خط Actual Average\nap1 = mpf.make_addplot(actual_average_df['Actual_Average'], type='line', color='red', linestyle='--', ax=ax, label='Actual Average')\n\n# اضافه کردن خط Predicted Average\nap2 = mpf.make_addplot(predicted_df['Predicted_Average'], type='line', color='blue', linestyle='-',  ax=ax, label='Predicted Average')\n\n# رسم کندل‌ها با هر دو خط\nmpf.plot(test_data, type='candle', style='charles', ax=ax, volume=False, addplot=[ap1, ap2])\n\n# اضافه کردن عنوان و لیبل‌ها\nax.set_title('Actual Candles with Actual vs Predicted Average (First 100 Samples of Test Data)')\nax.set_ylabel('Price')\n\n# نمایش لجند\nax.legend()\n\n# تنظیمات نهایی و نمایش نمودار\nfig.tight_layout()\nmpf.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}